{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RNN\n",
    "Recurrent neural networks (RNN) are the state of the art algorithm for sequential data and are used by Apple's Siri and and Google's voice search.  \n",
    "It is the first algorithm that remembers its input, due to an internal memory, which makes it perfectly suited for machine learning problems that involve sequential data. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "RNNs are a powerful and robust type of neural network, and belong to the most promising algorithms in use because it is the only one with an internal memory.  \n",
    "\n",
    "Because of their internal memory, RNN’s can remember important things about the input they received, which allows them to be very precise in predicting what’s coming next. This is why they're the preferred algorithm for sequential data like time series, speech, text, financial data, audio, video, weather and much more. Recurrent neural networks can form a much deeper understanding of a sequence and its context compared to other algorithms.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Simply put: **recurrent neural networks produce predictive results in sequential data that other algorithms can’t.**  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**But when do you need to use a RNN?**  \n",
    "Whenever there is a sequence of data and that temporal dynamics that connects the data is more important than the spatial content of each individual frame."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RNN vs. Feed-Forward Neural Networks\n",
    "In a feed-forward neural network, the information only moves in one direction — from the input layer, through the hidden layers, to the output layer. The information moves straight through the network and never touches a node twice.  \n",
    "\n",
    "In a RNN the information cycles through a loop. When it makes a decision, it considers the current input and also what it has learned from the inputs it received previously.  \n",
    "<img src=\"https://builtin.com/sites/default/files/styles/ckeditor_optimize/public/inline-images/rnn-vs-fnn.png\" height=300 width=500>  \n",
    "\n",
    "A usual RNN has a short-term memory. In combination with a LSTM they also have a long-term memory.  \n",
    "\n",
    "<a href=\"https://builtin.com/data-science/recurrent-neural-networks-and-lstm\">(source)</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example\n",
    "Consider something like a sentence:  \n",
    "*some people made a neural network*  \n",
    "\n",
    "Then, let's say we tokenized (split by) that sentence by word, and each word was a feature.  \n",
    "\n",
    "Feeding through a regular neural network, the above sentence would carry no more meaning that, say:  \n",
    "*a neural network made some people*  \n",
    "\n",
    "Obviously, these two sentences have widely varying impacts and meanings!  \n",
    "\n",
    "This is where recurrent neural networks come into play. They attempt to retain some of the importance of sequential data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With a Recurrent Neural Network, your input data is passed into a cell, which, along with outputting the activiation function's output, we take that output and include it as an input back into this cell.  \n",
    "<img src=\"https://pythonprogramming.net/static/images/machine-learning/basic-recurrent-neural-network-unfolded-concept.png\" height=300 width=500>  \n",
    "\n",
    "This can work, but this means we have a new set of problems: How should we weight incoming new data? How should we handle the recurring data? How should we handle/weight the relationship of the new data to the recurring data? What about as we continue down the line? If we're not careful, that initial signal could dominate everything down the line.  \n",
    "\n",
    "This is where the Long Short Term Memory (LSTM) Cell comes in. An LSTM cell looks like:  \n",
    "<img src=\"https://pythonprogramming.net/static/images/machine-learning/long-short-term-memory-cell-LSTM.png\" height=300 width=500>  \n",
    "\n",
    "The idea here is that we can have some sort of functions for determining what to forget from previous cells, what to add from the new input data, what to output to new cells, and what to actually pass on to the next layer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Learning rate\n",
    "\n",
    "Learning rate is a hyper-parameter that controls how much we are adjusting the weights of our network with respect the loss gradient. The lower the value, the slower we travel along the downward slope. While this might be a good idea (using a low learning rate) in terms of making sure that we do not miss any local minima, it could also mean that we’ll be taking a long time to converge — especially if we get stuck on a plateau region.  \n",
    "\n",
    "Typically learning rates are configured naively at random by the user. At best, the user would leverage on past experiences (or other types of learning material) to gain the intuition on what is the best value to use in setting learning rates.  \n",
    "\n",
    "As such, it’s often hard to get it right. The below diagram demonstrates the different scenarios one can fall into when configuring the learning rate.\n",
    "<img src=\"https://miro.medium.com/max/918/0*uIa_Dz3czXO5iWyI.\" height=300 width=500>  \n",
    "\n",
    "<a href=\"https://towardsdatascience.com/understanding-learning-rates-and-how-it-improves-performance-in-deep-learning-d0d4059c1c10\">(source)</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Decay\n",
    "As we know, learning rate is a parameter that determines how much an updating step influences the current value of the weights. While **weight decay** is an additional term in the weight update rule that causes the weights to exponentially decay to zero, if no other update is scheduled."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
