{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RNN with Cryptocurrency data\n",
    "We're going to work on using a recurrent neural network to predict against a time-series dataset, which is going to be cryptocurrency prices.  \n",
    "\n",
    "The data we'll be using is `Open`, `High`, `Low`, `Close`, `Volume` data for *Bitcoin*, *Ethereum*, *Litecoin* and *Bitcoin Cash*.  \n",
    "(Dataset can be downloaded from <a href=\"https://pythonprogramming.net/static/downloads/machine-learning-data/crypto_data.zip\">here</a>.)\n",
    "\n",
    "For our purposes here, we're going to only be focused on the Close and Volume columns.  \n",
    "The **Close** column measures the final price at the end of each interval. In this case, these are 1 minute intervals. So, at the end of each minute, what was the price of the asset.  \n",
    "The **Volume** column is how much of the asset was traded per each interval, in this case, per 1 minute.  \n",
    "\n",
    "We're going to be tracking the Close and Volume every minute for Bitcoin, Litecoin, Ethereum, and Bitcoin Cash."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Idea\n",
    "The idea is that these cryptocoins all have relationships with each other. Could we possibly predict future movements of, say, Litecoin, by analyzing the last 60 minutes of prices and volumes for all 4 of these coins? We would start with a guess that there exists some (at least better than random) relationship here that a recurrent neural network could discover."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PART 1: Dealing with Data\n",
    "### Input and Target\n",
    "Our data isn't already in some beautiful format where we have sequences mapped to targets. In fact, there are no targets at all. It's just some datapoints every 60 seconds.  \n",
    "\n",
    "- First, we need to combine price and volume for each coin into a single featureset, then we want to take these featuresets and combine them into sequences of 60 of these featuresets. This will be our input.  \n",
    "- Next, we'll be trying to predict if the price will rise or fall. So, we need to take the \"prices\" of the item we're trying to predict. Let's stick with saying we're trying to predict the price of Litecoin. So we need to grab the future price of Litecoin, then determine if it's higher or lower to the current price. We need to do this at every step."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Besides deciding on input and output, we also need to:\n",
    "1. Balance the dataset between buys and sells. We can also use class weights, but balance is superior.\n",
    "2. Scale/normalize the data in some way.\n",
    "3. Create reasonable out of sample data that works with the problem.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets import everything we may need"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets look at our data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"data/LTC-USD.csv\", names=['time', 'low', 'high', 'open', 'close', 'volume'])\n",
    "# print(df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What we want to do is somehow take the close and volume from here, and combine it with the other 3 cryptocurrencies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "main_df = pd.DataFrame()\n",
    "\n",
    "currencies = [\"BTC-USD\", \"LTC-USD\", \"BCH-USD\", \"ETH-USD\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets go through each csv file, rename the columns with specific currency types and ignore all the columns other than **close** and **volume**. And then, we will join all of them into single dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BTC-USD\n",
      "LTC-USD\n",
      "BCH-USD\n",
      "ETH-USD\n"
     ]
    }
   ],
   "source": [
    "for currency in currencies:\n",
    "    print(currency)\n",
    "    dataset = f\"data/{currency}.csv\"\n",
    "    df = pd.read_csv(dataset, names=[\"time\", \"low\", \"high\", \"open\", \"close\", \"volume\"])\n",
    "    df.rename(columns={\"close\": f\"{currency}_close\", \"volume\": f\"{currency}_volume\"}, inplace=True)\n",
    "    df.set_index(\"time\", inplace=True)\n",
    "    df = df[[f\"{currency}_close\", f\"{currency}_volume\"]]\n",
    "    \n",
    "    if len(main_df) == 0:\n",
    "        main_df = df\n",
    "    else:\n",
    "        main_df = main_df.join(df)  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets drop **NA** values and see the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>BTC-USD_close</th>\n",
       "      <th>BTC-USD_volume</th>\n",
       "      <th>LTC-USD_close</th>\n",
       "      <th>LTC-USD_volume</th>\n",
       "      <th>BCH-USD_close</th>\n",
       "      <th>BCH-USD_volume</th>\n",
       "      <th>ETH-USD_close</th>\n",
       "      <th>ETH-USD_volume</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>time</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1528968720</th>\n",
       "      <td>6487.379883</td>\n",
       "      <td>7.706374</td>\n",
       "      <td>96.660004</td>\n",
       "      <td>314.387024</td>\n",
       "      <td>870.859985</td>\n",
       "      <td>26.856577</td>\n",
       "      <td>486.01001</td>\n",
       "      <td>26.019083</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1528968780</th>\n",
       "      <td>6479.410156</td>\n",
       "      <td>3.088252</td>\n",
       "      <td>96.570000</td>\n",
       "      <td>77.129799</td>\n",
       "      <td>870.099976</td>\n",
       "      <td>1.124300</td>\n",
       "      <td>486.00000</td>\n",
       "      <td>8.449400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1528968840</th>\n",
       "      <td>6479.410156</td>\n",
       "      <td>1.404100</td>\n",
       "      <td>96.500000</td>\n",
       "      <td>7.216067</td>\n",
       "      <td>870.789978</td>\n",
       "      <td>1.749862</td>\n",
       "      <td>485.75000</td>\n",
       "      <td>26.994646</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1528968900</th>\n",
       "      <td>6479.979980</td>\n",
       "      <td>0.753000</td>\n",
       "      <td>96.389999</td>\n",
       "      <td>524.539978</td>\n",
       "      <td>870.000000</td>\n",
       "      <td>1.680500</td>\n",
       "      <td>486.00000</td>\n",
       "      <td>77.355759</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1528968960</th>\n",
       "      <td>6480.000000</td>\n",
       "      <td>1.490900</td>\n",
       "      <td>96.519997</td>\n",
       "      <td>16.991997</td>\n",
       "      <td>869.989990</td>\n",
       "      <td>1.669014</td>\n",
       "      <td>486.00000</td>\n",
       "      <td>7.503300</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            BTC-USD_close  BTC-USD_volume  LTC-USD_close  LTC-USD_volume  \\\n",
       "time                                                                       \n",
       "1528968720    6487.379883        7.706374      96.660004      314.387024   \n",
       "1528968780    6479.410156        3.088252      96.570000       77.129799   \n",
       "1528968840    6479.410156        1.404100      96.500000        7.216067   \n",
       "1528968900    6479.979980        0.753000      96.389999      524.539978   \n",
       "1528968960    6480.000000        1.490900      96.519997       16.991997   \n",
       "\n",
       "            BCH-USD_close  BCH-USD_volume  ETH-USD_close  ETH-USD_volume  \n",
       "time                                                                      \n",
       "1528968720     870.859985       26.856577      486.01001       26.019083  \n",
       "1528968780     870.099976        1.124300      486.00000        8.449400  \n",
       "1528968840     870.789978        1.749862      485.75000       26.994646  \n",
       "1528968900     870.000000        1.680500      486.00000       77.355759  \n",
       "1528968960     869.989990        1.669014      486.00000        7.503300  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# if there are gaps in data, use previously known values\n",
    "main_df.fillna(method=\"ffill\", inplace=True)\n",
    "# and then drop invalid values if any\n",
    "main_df.dropna(inplace=True)\n",
    "main_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we need to create a target. To do this, we need to know which price we're trying to predict. We also need to know how far out we want to predict.   \n",
    "\n",
    "We'll choose Litecoin for now. Knowing how far out we want to predict probably also depends how long our sequences are. If our sequence length is 3 (3 minutes), we probably can't easily predict out 10 minutes. If our sequence length is 300, 10 might not be as hard. So we'll start with a sequence length of 60, and a future prediction out of 3.  \n",
    "\n",
    "We could also make the prediction a regression question, using a linear activation with the output layer, but, instead, Iwe will just go with a binary classification.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If price goes up in 3 minutes, then it's a buy. If it goes down in 3 minutes, not buy/sell. With all of that in mind, lets define few constants:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "SEQ_LEN = 60  # how long of a preceeding sequence to collect for RNN\n",
    "FUTURE_PERIOD_PREDICT = 3  # how far into the future are we trying to predict?\n",
    "CURRENCY_TO_PREDICT = \"LTC-USD\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we will define a simple classification function that we'll use in future:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def classify(current , future):\n",
    "    if float(future) > float(current):\n",
    "        return 1\n",
    "    else:\n",
    "        return 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This function will take values from 2 columns. If the \"future\" column is higher, it's a 1 (buy). Otherwise it's a 0 (sell).   \n",
    "To do this, first, we need a future column!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "main_df['future'] = main_df[f\"{CURRENCY_TO_PREDICT}_close\"].shift(-FUTURE_PERIOD_PREDICT)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A `shift` will just shift the columns for us, a negative shift will shift them \"up.\" So shifting up 3 will give us the price 3 minutes in the future, and we're just assigning this to a new column."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we've got the future values, we can use them to make a target using the function we defined above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "main_df['target'] = list(map(classify, main_df[f\"{CURRENCY_TO_PREDICT}_close\"], main_df['future']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `map()` is used to map a function. The first parameter here is the function we want to map (classify), then the next ones are the parameters to that function. In this case, the current close price, and then the future price.  \n",
    "The map part is what allows us to do this row-by-row for these columns, but also do it quite fast. The list part converts the end result to a list, which we can just set as a column."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now lets check out our data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>BTC-USD_close</th>\n",
       "      <th>BTC-USD_volume</th>\n",
       "      <th>LTC-USD_close</th>\n",
       "      <th>LTC-USD_volume</th>\n",
       "      <th>BCH-USD_close</th>\n",
       "      <th>BCH-USD_volume</th>\n",
       "      <th>ETH-USD_close</th>\n",
       "      <th>ETH-USD_volume</th>\n",
       "      <th>future</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>time</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1528968720</th>\n",
       "      <td>6487.379883</td>\n",
       "      <td>7.706374</td>\n",
       "      <td>96.660004</td>\n",
       "      <td>314.387024</td>\n",
       "      <td>870.859985</td>\n",
       "      <td>26.856577</td>\n",
       "      <td>486.01001</td>\n",
       "      <td>26.019083</td>\n",
       "      <td>96.389999</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1528968780</th>\n",
       "      <td>6479.410156</td>\n",
       "      <td>3.088252</td>\n",
       "      <td>96.570000</td>\n",
       "      <td>77.129799</td>\n",
       "      <td>870.099976</td>\n",
       "      <td>1.124300</td>\n",
       "      <td>486.00000</td>\n",
       "      <td>8.449400</td>\n",
       "      <td>96.519997</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1528968840</th>\n",
       "      <td>6479.410156</td>\n",
       "      <td>1.404100</td>\n",
       "      <td>96.500000</td>\n",
       "      <td>7.216067</td>\n",
       "      <td>870.789978</td>\n",
       "      <td>1.749862</td>\n",
       "      <td>485.75000</td>\n",
       "      <td>26.994646</td>\n",
       "      <td>96.440002</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1528968900</th>\n",
       "      <td>6479.979980</td>\n",
       "      <td>0.753000</td>\n",
       "      <td>96.389999</td>\n",
       "      <td>524.539978</td>\n",
       "      <td>870.000000</td>\n",
       "      <td>1.680500</td>\n",
       "      <td>486.00000</td>\n",
       "      <td>77.355759</td>\n",
       "      <td>96.470001</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1528968960</th>\n",
       "      <td>6480.000000</td>\n",
       "      <td>1.490900</td>\n",
       "      <td>96.519997</td>\n",
       "      <td>16.991997</td>\n",
       "      <td>869.989990</td>\n",
       "      <td>1.669014</td>\n",
       "      <td>486.00000</td>\n",
       "      <td>7.503300</td>\n",
       "      <td>96.400002</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            BTC-USD_close  BTC-USD_volume  LTC-USD_close  LTC-USD_volume  \\\n",
       "time                                                                       \n",
       "1528968720    6487.379883        7.706374      96.660004      314.387024   \n",
       "1528968780    6479.410156        3.088252      96.570000       77.129799   \n",
       "1528968840    6479.410156        1.404100      96.500000        7.216067   \n",
       "1528968900    6479.979980        0.753000      96.389999      524.539978   \n",
       "1528968960    6480.000000        1.490900      96.519997       16.991997   \n",
       "\n",
       "            BCH-USD_close  BCH-USD_volume  ETH-USD_close  ETH-USD_volume  \\\n",
       "time                                                                       \n",
       "1528968720     870.859985       26.856577      486.01001       26.019083   \n",
       "1528968780     870.099976        1.124300      486.00000        8.449400   \n",
       "1528968840     870.789978        1.749862      485.75000       26.994646   \n",
       "1528968900     870.000000        1.680500      486.00000       77.355759   \n",
       "1528968960     869.989990        1.669014      486.00000        7.503300   \n",
       "\n",
       "               future  target  \n",
       "time                           \n",
       "1528968720  96.389999       0  \n",
       "1528968780  96.519997       0  \n",
       "1528968840  96.440002       0  \n",
       "1528968900  96.470001       1  \n",
       "1528968960  96.400002       0  "
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "main_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that *future* and *target* columns are specific only to currency we chose to predict in the beginning."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PART 2: Normalizing and creating Sequences"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first thing we want to do is separate out our validation(out of sample) data.  \n",
    "\n",
    "In the past, all we did was shuffle data, then slice it. But the problem with that method is, the data is inherently sequential, so taking sequences that don't come in the future is likely a mistake. This is because sequences in our case, for example, 1 minute apart, will be almost identical. Chances are, the target is also going to be the same (buy or sell). Because of this, any overfitting is likely to actually pour over into the validation set.  \n",
    "\n",
    "Instead, we want to slice our validation while it's still in order.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So lets take the last 5% of the data as validation data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1534922100\n"
     ]
    }
   ],
   "source": [
    "# get the times\n",
    "times = sorted(main_df.index.values)\n",
    "# get the last 5% of the times\n",
    "last_5pct = sorted(main_df.index.values)[-int(0.05 * len(times))]\n",
    "# this is the timestamp from where we want to seperate validation data\n",
    "print(last_5pct)\n",
    "\n",
    "# make the validation data where the index is in the last 5%\n",
    "validation_main_df = main_df[(main_df.index >= last_5pct)]\n",
    "# now the main_df is all the data up to the last 5%\n",
    "main_df = main_df[(main_df.index < last_5pct)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we need to balance and normalize this data. By balance, we want to make sure the classes have equal amounts when training, so our model doesn't just always predict one class.  \n",
    "\n",
    "One way to counteract this is to use class weights, which allows you to weight loss higher for lesser-frequent classifications.  \n",
    "\n",
    "We also need to take our data and make sequences from it.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Let's start by removing the future column (the actual target is called literally target and only needed the future column temporarily to create it). \n",
    "- Then we'll Normalize the data and Acale it too\n",
    "- Next, we will create sequences of a fixed time period (say 60 minutes each) from the data.\n",
    "- Next, we need to balance the dataset. (For a Cats vs Dogs classifier, it is better to have equal amount of Cat images and Dog images. Similarly, in this scenario, it is better to balance the dataset with equal amount of buys and sells - based on target).\n",
    "- Finally lets separate features and labels and pass them back to caller."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import preprocessing\n",
    "from collections import deque\n",
    "import random\n",
    "\n",
    "def preprocess_df(df):\n",
    "    df = df.drop(\"future\", 1)\n",
    "    \n",
    "    ## NORMALIZING AND SCALING ##\n",
    "    for col in df.columns:\n",
    "        # normalize all columns for the target column\n",
    "        if col != \"target\":\n",
    "            # pct change \"normalizes\" the different currencies \n",
    "            # each crypto coin has vastly different values, \n",
    "            # so we're really more interested in the other coin's movements)\n",
    "            df[col] = df[col].pct_change()\n",
    "            # remove the NA's created by pct_change\n",
    "            df.dropna(inplace=True)\n",
    "            # scale between 0 and 1\n",
    "            df[col] = preprocessing.scale(df[col].values)\n",
    "           \n",
    "    # cleanup again (just in case)\n",
    "    df.dropna(inplace=True)\n",
    "    \n",
    "    ## CREATING THE SEQUENCES ##\n",
    "    \n",
    "    # this is a list that will contain the sequences\n",
    "    sequential_data = []\n",
    "    # these will be our actual sequences\n",
    "    # they are made with deque, which keeps the maximum length by popping out older values as new ones come in\n",
    "    prev_minutes = deque(maxlen=SEQ_LEN)\n",
    "\n",
    "    # iterate over the values (rows)\n",
    "    for i in df.values:\n",
    "        # store everything in the row, except target\n",
    "        prev_minutes.append([n for n in i[:-1]])\n",
    "        if len(prev_minutes) == SEQ_LEN:\n",
    "            sequential_data.append([np.array(prev_minutes), i[-1]])\n",
    "\n",
    "    # shuffle for good measure\n",
    "    random.shuffle(sequential_data)\n",
    "    \n",
    "    ## BALANCING THE DATASET ##\n",
    "    # list that will store our buy sequences and targets\n",
    "    buys = []\n",
    "    # list that will store our sell sequences and targets\n",
    "    sells = []\n",
    "    \n",
    "    # split the dataset into buys and sells based on target\n",
    "    for seq, target in sequential_data:\n",
    "        if target == 0:\n",
    "            sells.append([seq, target])\n",
    "        elif target == 1:\n",
    "            buys.append([seq, target])\n",
    "       \n",
    "    # shuffle them both\n",
    "    random.shuffle(buys)\n",
    "    random.shuffle(sells)\n",
    "    \n",
    "    # find out which is smaller, buys or sells\n",
    "    lower = min(len(buys), len(sells))\n",
    "    \n",
    "    # them limit each of them to that 'lower' number\n",
    "    buys = buys[:lower]\n",
    "    sells = sells[:lower]\n",
    "    \n",
    "    # add them both and shuffle them again\n",
    "    sequential_data = buys + sells\n",
    "    random.shuffle(sequential_data)\n",
    "    \n",
    "    \n",
    "    X = []\n",
    "    y = []\n",
    "    \n",
    "    for seq, target in sequential_data:\n",
    "        X.append(seq)\n",
    "        y.append(target)\n",
    "        \n",
    "    return np.array(X), np.array(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets preprocess our data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_x, train_y = preprocess_df(main_df)\n",
    "validation_x, validation_y = preprocess_df(validation_main_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's print some stats real quick to make sure things are what we expect:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Data Size: 77922 \n",
      "Validation Data Size: 3860\n"
     ]
    }
   ],
   "source": [
    "print(f\"Training Data Size: {len(train_x)} \\nValidation Data Size: {len(validation_x)}\")\n",
    "# print(f\"Training data:\\nSells: {train_y.count(0)} | Buys: {train_y.count(1)}\")\n",
    "# print(f\"Validation data:\\nSells : {validation_y.count(0)} | Buys: {validation_y.count(1)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, lets start with tensorflow stuff.  \n",
    "Lets import all the necessary things (models, layers and callbacks) and define some constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout, LSTM, BatchNormalization\n",
    "from tensorflow.keras.callbacks import TensorBoard\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "# how many passes through our data\n",
    "EPOCHS = 10\n",
    "# number of batches (try smaller batch if you're getting OOM - out of memory errors\n",
    "BATCH_SIZE = 64\n",
    "# a unique name for the model\n",
    "NAME = f\"{CURRENCY_TO_PREDICT}-{SEQ_LEN}-SEQ-{FUTURE_PERIOD_PREDICT}-PRED-{int(time.time())}\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets build our model.  \n",
    "\n",
    "We will start with a Sequential model, add few LSTM layers and a Dense layer and finally an output layer. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "\n",
    "model.add(LSTM(128, input_shape=(train_x.shape[1:]), return_sequences=True))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(BatchNormalization())    #normalizes activation outputs\n",
    "\n",
    "model.add(LSTM(128, activation='relu', return_sequences=True))\n",
    "model.add(Dropout(0.1))\n",
    "model.add(BatchNormalization())    #normalizes activation outputs\n",
    "\n",
    "model.add(LSTM(128, activation='relu'))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(BatchNormalization())    #normalizes activation outputs\n",
    "\n",
    "model.add(Dense(32, activation='relu'))\n",
    "model.add(Dropout(0.2))\n",
    "\n",
    "model.add(Dense(2, activation='softmax'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets choose an optimizer and compile our model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "opt = tf.keras.optimizers.Adam(lr=0.001, decay=1e-6)\n",
    "\n",
    "model.compile(\n",
    "    loss='sparse_categorical_crossentropy',\n",
    "    optimizer=opt,\n",
    "    metrics=['accuracy']\n",
    ")   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, lets add a Tensorflow callback for futuer analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "tensorboard = TensorBoard(log_dir=\"logs/{}\".format(NAME))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets also add the ModelCheckpoint callback for saving best models with good accuracy for future use"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint_filepath = \"models/RNN_Final-{epoch:02d}-{val_accuracy:.3f}.hd5\"\n",
    "# saves only the best models\n",
    "checkpoint = ModelCheckpoint(filepath=checkpoint_filepath, monitor='val_accuracy', verbose=1, save_best_only=True, mode='max')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, finally, lets train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "1218/1218 [==============================] - ETA: 0s - loss: 0.6930 - accuracy: 0.5118\n",
      "Epoch 00001: val_accuracy improved from -inf to 0.50648, saving model to models/RNN_Final-01-0.506.hd5\n",
      "INFO:tensorflow:Assets written to: models/RNN_Final-01-0.506.hd5\\assets\n",
      "1218/1218 [==============================] - 458s 376ms/step - loss: 0.6930 - accuracy: 0.5118 - val_loss: 0.7038 - val_accuracy: 0.5065\n",
      "Epoch 2/10\n",
      "1218/1218 [==============================] - ETA: 0s - loss: 0.6929 - accuracy: 0.5125\n",
      "Epoch 00002: val_accuracy did not improve from 0.50648\n",
      "1218/1218 [==============================] - 446s 366ms/step - loss: 0.6929 - accuracy: 0.5125 - val_loss: 0.6933 - val_accuracy: 0.5003\n",
      "Epoch 3/10\n",
      "1218/1218 [==============================] - ETA: 0s - loss: 0.6929 - accuracy: 0.5106\n",
      "Epoch 00003: val_accuracy did not improve from 0.50648\n",
      "1218/1218 [==============================] - 452s 371ms/step - loss: 0.6929 - accuracy: 0.5106 - val_loss: 0.6945 - val_accuracy: 0.5003\n",
      "Epoch 4/10\n",
      "1218/1218 [==============================] - ETA: 0s - loss: 0.6928 - accuracy: 0.5115\n",
      "Epoch 00004: val_accuracy improved from 0.50648 to 0.51606, saving model to models/RNN_Final-04-0.516.hd5\n",
      "INFO:tensorflow:Assets written to: models/RNN_Final-04-0.516.hd5\\assets\n",
      "1218/1218 [==============================] - 467s 383ms/step - loss: 0.6928 - accuracy: 0.5115 - val_loss: 0.6929 - val_accuracy: 0.5161\n",
      "Epoch 5/10\n",
      "1218/1218 [==============================] - ETA: 0s - loss: 0.6926 - accuracy: 0.5149\n",
      "Epoch 00005: val_accuracy did not improve from 0.51606\n",
      "1218/1218 [==============================] - 451s 370ms/step - loss: 0.6926 - accuracy: 0.5149 - val_loss: 0.6926 - val_accuracy: 0.5016\n",
      "Epoch 6/10\n",
      "1218/1218 [==============================] - ETA: 0s - loss: 0.6929 - accuracy: 0.5102\n",
      "Epoch 00006: val_accuracy did not improve from 0.51606\n",
      "1218/1218 [==============================] - 492s 404ms/step - loss: 0.6929 - accuracy: 0.5102 - val_loss: 0.6929 - val_accuracy: 0.5016\n",
      "Epoch 7/10\n",
      "1218/1218 [==============================] - ETA: 0s - loss: 0.6929 - accuracy: 0.5085\n",
      "Epoch 00007: val_accuracy did not improve from 0.51606\n",
      "1218/1218 [==============================] - 510s 418ms/step - loss: 0.6929 - accuracy: 0.5085 - val_loss: 0.6935 - val_accuracy: 0.5060\n",
      "Epoch 8/10\n",
      "1218/1218 [==============================] - ETA: 0s - loss: 0.6928 - accuracy: 0.5112\n",
      "Epoch 00008: val_accuracy did not improve from 0.51606\n",
      "1218/1218 [==============================] - 477s 391ms/step - loss: 0.6928 - accuracy: 0.5112 - val_loss: 0.6935 - val_accuracy: 0.5010\n",
      "Epoch 9/10\n",
      "1218/1218 [==============================] - ETA: 0s - loss: 0.6927 - accuracy: 0.5117\n",
      "Epoch 00009: val_accuracy did not improve from 0.51606\n",
      "1218/1218 [==============================] - 473s 389ms/step - loss: 0.6927 - accuracy: 0.5117 - val_loss: 0.6937 - val_accuracy: 0.5013\n",
      "Epoch 10/10\n",
      "1218/1218 [==============================] - ETA: 0s - loss: 0.6927 - accuracy: 0.5155\n",
      "Epoch 00010: val_accuracy did not improve from 0.51606\n",
      "1218/1218 [==============================] - 474s 390ms/step - loss: 0.6927 - accuracy: 0.5155 - val_loss: 0.6929 - val_accuracy: 0.5142\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(\n",
    "    train_x, train_y,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    epochs=EPOCHS,\n",
    "    validation_data=(validation_x, validation_y),\n",
    "    callbacks=[tensorboard, checkpoint]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets visualize the accuracy and loss using Tensorboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ERROR: Timed out waiting for TensorBoard to start. It may still be running as pid 644."
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%load_ext tensorboard\n",
    "import datetime, os\n",
    "\n",
    "logs_base_dir = \"./logs\"\n",
    "os.makedirs(logs_base_dir, exist_ok=True)\n",
    "%tensorboard --logdir {logs_base_dir}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After executing above cell, got to localhost:6006 to see the output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
