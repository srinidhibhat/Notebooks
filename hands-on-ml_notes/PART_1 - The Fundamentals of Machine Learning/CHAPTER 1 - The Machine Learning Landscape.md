## CHAPTER 1: The Machine Learning Landscape

- Applying ML techniques to dig into large amounts of data can help discover patterns that were not immediately apparent. This is called data mining. 
- In Machine Learning an attribute is a data type (e.g., “Mileage”), while a feature has several meanings depending on the context, but generally means an attribute plus its value (e.g., “Mileage = 15,000”). Many people use the words attribute and feature interchangeably, though.
- Dimensionality reduction is a task in which the goal is to simplify the data without losing too much information. One way to do this is to merge several correlated features into one. For example, a car’s mileage may be very correlated with its age, so the dimensionality reduction algorithm will merge them into one feature that represents the car’s wear and tear. This is called 'feature extraction'.
- Reinforcement Learning is a very different beast. The learning system, called an agent in this context, can observe the environment, select and perform actions, and get rewards in return (or penalties in the form of negative rewards). It must then learn by itself what is the best strategy, called a policy, to get the most reward over time. A policy defines what action the agent should choose when it is in a given situation.
- In batch learning, the system is incapable of learning incrementally: it must be trained using all the available data. This will generally take a lot of time and computing resources, so it is typically done offline. First the system is trained, and then it is launched into production and runs without learning anymore; it just applies what it has learned. This is called offline learning.
- In online learning, you train the system incrementally by feeding it data instances sequentially, either individually or by small groups called mini-batches. Each learning step is fast and cheap, so the system can learn about new data on the fly, as it arrives.
- One important parameter of online learning systems is how fast they should adapt to changing data: this is called the learning rate. If you set a high learning rate, then your system will rapidly adapt to new data, but it will also tend to quickly forget the old data.
- One more way to categorize Machine Learning systems is by how they generalize.There are two main approaches to generalization: 
    1. Instance-based learning: Possibly the most trivial form of learning is simply to learn by heart. If you were to create a spam filter this way, it would just flag all emails that are identical to emails that have already been flagged by users—not the worst solution, but certainly not the best.
    2. Model-based learning: Another way to generalize from a set of  examples is to build a model of these examples, then use that model to make predictions. This is called model-based learning.
- How can you know which values will make your model perform best? To answer this question, you need to specify a performance measure. You can either define a utility function (or fitness function) that measures how good your model is, or you can define a cost function that measures how bad it is.
- Main Challenges of Machine Learning:
    - Insufficient Quantity of Training Data: Sometimes corpus development is more important than algorithm development.
    - Nonrepresentative Training Data: In order to generalize well, it is crucial that your training data be representative of the new cases you want to generalize to. This is often harder than it sounds: if the sample is too small, you will have sampling noise, but even very large samples can be nonrepresentative if the sampling method is flawed. This is called 'sampling bias'.
    - Poor-Quality Data: Obviously, if your training data is full of errors, outliers, and noise, it will make it harder for the system to detect the underlying patterns, so your system is less likely to perform well.
    - Irrelevant Features: A critical part of the success of a Machine Learning project is coming up with a good set of features to train on. This process, called 'feature engineering', involves: Feature selection, Feature extraction (combining existing features to produce a more useful one) and Creating new features by gathering new data.
    - Overfitting the Training Data: It means that the model performs well on the training data, but it does not generalize well. Constraining a model to make it simpler and reduce the risk of overfitting is called 'regularization'. The amount of regularization to apply during learning can be controlled by a 'hyperparameter'. A hyperparameter is a parameter of a learning algorithm (not of the model).
    - Underfitting the Training Data: It occurs when your model is too simple to learn the underlying structure of the data.

- Testing and Validating: You train multiple models with various  hyperparameters using the training set, you select the model and hyperparameters that perform best on the validation set, and when you’re  happy with your model you run a single final test against the test set to get an estimate of the generalization error. To avoid “wasting” too much training data in validation sets, a common technique is to use 'cross-validation': the training set is split into complementary subsets, and each model is trained against a different combination of these subsets and validated against the remaining parts. Once the model type and hyperparameters have been selected, a final model is trained using these hyperparameters on the full training set, and the generalized error is measured on the test set.
