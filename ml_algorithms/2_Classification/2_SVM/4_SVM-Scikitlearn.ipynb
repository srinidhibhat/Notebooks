{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## OVR\n",
    "SVM is a binary classifier. This means, at any one time, the SVM optimization is really tasked to separate one group from another.  \n",
    "The question is then how we might classify a total of 3 or more groups. Typically, the method is to do what is referred to as \"One Verse Rest\" or (OVR). The idea here is you separate each group from the rest.  \n",
    "For example, to classify three separate groups (1, 2, and 3), you would start by separating 1 from 2 and 3. Then you would separate 2 from 1 and 3. Then finally separate 3 from 1 and 2. There are some issues with this, as things like confidence may be different per classification boundary, also the separation boundaries may be slightly flawed since there are almost always going to be more negatives than positives, since you're maybe comparing one group to three others. Assuming a balanced dataset at the start, this would mean every classification boundary is actually unbalanced.\n",
    "<img src=\"https://pythonprogramming.net/static/images/machine-learning/one-vs-rest-svm.png\" height=400 width=700>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## OVO\n",
    "Another method is One-vs-One (or OVO). In this case, consider you have three total groups. The way this works is you have a specific boundary that separates 1 from 3 and 1 from 2, and this process repeats for the rest of the classes. In this way, the boundaries may be more balanced.  \n",
    "<img src=\"https://pythonprogramming.net/static/images/machine-learning/one-vs-one-svm.png\" height=400 width=700>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SVM Parameters and attributes\n",
    "When using SVM from scikit-learn, there are lot of parameters that we can pass (or use the default values). It is a good idea to know about how these different parameters affect the performance of the classifier.  \n",
    "Some important parameters are:  \n",
    "- C: This tells you right away that this is a soft-margin classifier. You can adjust C however you like, and you could make C high enough to create a hard-margin classifier.  \n",
    "- kernel: The default here is the rbf kernel, but you can also just have a linear kernel, a poly (for polynomial), sigmoid, or even a custom one of your choosing or design.  \n",
    "- probability: Recall how an algorithm like K Nearest Neighbors not only has a model accuracy, but also each prediction can have a degree of \"confidence.\" The SVM doesn't inherently have an attribute like this, but you can use this probability parameter to enable a form of one. This is a costly functionality, but may be important enough to you to enable it, otherwise the default is False.  \n",
    "- tol: setting for the SVM's tolerance in optimization. Recall that yi(xi.w+b)-1 >= 0. For an SVM to be valid, all values must be greater than or equal to 0, and at least one value on each side needs to be \"equal\" to 0, which will be your support vectors. Since it is highly unlikely that you will actually get values equal perfectly to 0, you set tolerance to allow a bit of wiggle room. The default tol with Scikit-Learn's SVM is 1e-3, which is 0.001.  \n",
    "- max_iter: maximum number of iterations for the quadratic programming problem to cycle through to optimize. The default is -1, which means there is no limit.\n",
    "- decision_function_shape: one-vs-one (ovo) or one-vs-rest (ovr)  \n",
    "<br><br>\n",
    "We also have a few **attributes**:\n",
    "- support_: gives you the index values for the support vectors. \n",
    "- support_vectors_: are the actual support vectors. \n",
    "- n_support_: will tell you how many support vectors you have, which is useful for comparing to your dataset size to determine if you may have some statistical issues.\n",
    "<br>\n",
    "You can refer all of them <a href=\"https://scikit-learn.org/0.16/modules/generated/sklearn.svm.SVC.html\">here</a>."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
