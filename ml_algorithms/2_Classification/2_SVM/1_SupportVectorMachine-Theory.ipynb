{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Support Vector Machines\n",
    "\n",
    "Support vector machines are classification algorithms that divide a data set into categories based by slicing **through the widest gap between categories.**  \n",
    "\n",
    "The Support Vector Machine, in general, handles pointless data better than the K Nearest Neighbors algorithm, and definitely will handle outliers better. It also achieves similar accuracy, only at a much faster pace.  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Vector Basics\n",
    "- A vector has both a magnitude and a direction.\n",
    "- The dot product tells you what amount of one vector goes in the direction of another. Dot product here, is used in order to measure the distance from one example to the median of the street (line of separation between - and + examples).  \n",
    "\n",
    "More about vectors <a href=\"https://www.svm-tutorial.com/2014/11/svm-understanding-math-part-2/\">here</a>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How Do Support Vector Machines Work?\n",
    "Given a set of training examples – each of which is marked for belonging to one of two categories – a support vector machine training algorithm builds a model. This model assigns new examples into one of the two categories. This makes the support vector machine a non-probabilistic binary linear classifier.\n",
    "\n",
    "More specifically, an SVM model maps the data points as points in space and divides the separate categories so that they are divided by an open gap that is as wide as possible. New data points are predicted to belong to a category based on which side of the gap they belong to.\n",
    "<br>\n",
    "For example consider this visualization:\n",
    "<img src=\"https://www.freecodecamp.org/news/content/images/2020/06/image-57.png\">\n",
    "Here, if a new data point falls on the left side of the green line, it will be labeled with the red category. Similarly, if a new data point falls on the right side of the green line, it will get labelled as belonging to the blue category.\n",
    "This green line is called a **hyperplane**, which is an important piece of vocabulary for support vector machine algorithms.\n",
    "\n",
    "### What is a hyperplane?\n",
    "Hyperplane is just a line in more that 3 dimensions\n",
    "<img src=\"https://miro.medium.com/max/1280/1*H2QEWsP9-W4rBdIaxfVExg.jpeg\">\n",
    "<br>\n",
    "Let’s take a look at a different visual representation of a support vector machine:\n",
    "<img src=\"https://www.freecodecamp.org/news/content/images/2020/06/image-58.png\">\n",
    "\n",
    "In this diagram, the hyperplane is labelled as the **optimal hyperplane**. Support vector machine theory defines the optimal hyperplane as the one that maximizes the margin between the closest data points from each category.\n",
    "\n",
    "As you can see, the margin line actually touches three data points – two from the red category and one from the blue category. These data points which touch the margin lines are called **support vectors** and are where support vector machines get their name from.  \n",
    "\n",
    "Once you find the support vectors, you want to create lines that are **maximally separated** between each other. From here, we can easily find the decision boundary by taking the total width and dividing it by 2.\n",
    "\n",
    "\n",
    "<a href=\"https://www.freecodecamp.org/news/a-no-code-intro-to-the-9-most-important-machine-learning-algorithms-today/\">source</a>\n",
    "\n",
    "### How could we define a hyperplane?\n",
    "\n",
    "Let’s look at the two-dimensional case first. The two-dimensional linearly separable data can be separated by a line. The function of the line is `y=ax+b`. We rename `x` with `x1` and `y` with `x2` and we get:\n",
    "`ax1−x2+b=0`\n",
    "\n",
    "If we define `x = (x1,x2)` and `w = (a,−1)`, we get:\n",
    "`w⋅x+b=0`\n",
    "\n",
    "This equation is derived from two-dimensional vectors. But in fact, it also works for any number of dimensions. This is the equation of the hyperplane.\n"
   ]
  },
  {
   "attachments": {
    "Capture.PNG": {
     "image/png": "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"
    }
   },
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classifier\n",
    "Once we have the hyperplane, we can then use the hyperplane to make predictions. We define the hypothesis function h as:  \n",
    "![Capture.PNG](attachment:Capture.PNG)\n",
    "\n",
    "The point above or on the hyperplane will be classified as class +1, and the point below the hyperplane will be classified as class -1.\n",
    "\n",
    "Or, we can simply say that the classification function is just: `sign(x.w + b)`  [sign being positive or negative]\n",
    "\n",
    "So basically, the goal of the SVM learning algorithm is to find a hyperplane which could separate the data accurately. There might be many such hyperplanes. And we need to find the best one, which is often referred as the optimal hyperplane.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SVM optimization problem\n",
    "Let’s first consider the equation of the hyperplane `w⋅x+b=0`. We know that if the point (x,y) is on the hyperplane, `w⋅x+b=0`. If the point (x,y) is not on the hyperplane, the value of `w⋅x+b` could be positive or negative. For all the training example points, we want to know the point which is closest to the hyperplane."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Except for the whole thing where we have no idea what `w` is and what `b` is. There are an infinite number of w's and b's that might satisfy our equation, but we want the \"best\" separating hyperplane. The best separating hyperplane is the one with the most width between the data it is separating. We can probably guess that's going to play a part in the optimization of w and b.  \n",
    "<br>\n",
    "Once we find a `w` and `b` to satisfy the constraint problem (the vector w with the smallest magnitude with the largest b), our decision function for classification of unknown points would just simply ask for the value of `x.w + b`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The SVM's optimization problem is a convex problem, where the convex shape is the magnitude of vector w:\n",
    "<img src=\"https://pythonprogramming.net/static/images/machine-learning/svm-convex-problem.png\">  \n",
    "<br>\n",
    "The objective of this convex problem is to find the minimum magnitude of vector w. One way to solve convex problems is by *\"stepping down\"* until you cannot get any further down. You will start with a large step, quickly getting down. Once you find the bottom, you are going to slowly go back the other way, repeating this until you find the absolute bottom. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SVM for Non-Linear Data Sets\n",
    "- Sometimes, we cannot find a straight line to separate the two categories of classification. In that case will use the **Kernel Trick!**  \n",
    "<br>\n",
    "- The basic idea is that when a data set is inseparable in the current dimensions, **add another dimension**, maybe that way the data will be separable.  \n",
    "<br>\n",
    "- To solve this problem we shouldn’t just **blindly add another dimension**, we should transform the space so we generate this level difference intentionally.  \n",
    "<br>\n",
    "- Let's assume that we add another dimension called `X3`. Another important transformation is that in the new dimension the points are organized using this formula `x1² + x2²`.  \n",
    "These transformations are called kernels. Popular kernels are: Polynomial Kernel, Gaussian Kernel, Radial Basis Function (RBF) etc.  \n",
    "(<a href=\"https://towardsdatascience.com/svm-and-kernel-svm-fed02bef1200\">source</a>)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Kernels are similarity functions, which take two inputs and return a similarity using inner products. Not only can you create your own new machine learning algorithms with Kernels, you can also translate existing machine learning algorithms into using Kernels.  \n",
    "\n",
    "What kernels are going to allow us to do, possibly, is work in many dimensions, without actually paying the processing costs to do it. Kernels do have a requirement: They rely on inner products. (\"dot product\" is same as \"inner product\").  \n",
    "\n",
    "What we need to do in order to verify whether or not we can get away with using kernels is confirm that every interaction with our featurespace is an inner product. We'll start at the end, and work our way back to confirm this.  \n",
    "So if you look at the equations of SVM `y = sign(x.w + b)`, we can use kernels. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Why Kernels?\n",
    "Generally, kernels will be defined by something like: k(x, x`)  \n",
    "<br>\n",
    "The kernel function is applied to x and x prime, and will equal the inner product of z and z prime, where the z values are from the z dimension (our new dimension space).\n",
    "The z values are the result of some function(x), and these z values are dotted together to give us our kernel function's result.\n",
    "<img src=\"https://pythonprogramming.net/static/images/machine-learning/z-values-kernel-function-of-x.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are quite a few pre-made kernels, but typically, the default is Radial Basis Function (RBF) kernel, since it can take us to a proposed \"infinite\" number of dimensions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Soft Margin Support Vector Machine\n",
    "First, there are two major reasons why the soft-margin classifier might be superior. One reason is our data is not perfectly linearly separable, but is very close and it makes more sense to continue using the default linearly kernel. The other reason is, even if you are using a kernel, you may wind up with significant **over-fitment** or **overfitting** if you want to use a hard-margin. For example, consider:\n",
    "<img src=\"https://pythonprogramming.net/static/images/machine-learning/example%20data-not-linearly-separable.png\">\n",
    "Assuming a hard-margin and the support vector hyperplanes, it looks like this:\n",
    "<img src=\"https://pythonprogramming.net/static/images/machine-learning/hard-margin-with-many-support-vectors.png\">\n",
    "In this case, every single data sample for the positive class is a support vector, and only two of the negative class aren't support vectors. This signals to use a high chance of overfitting having happened.  \n",
    "<br>\n",
    "What if we did something like this instead:\n",
    "<img src=\"https://pythonprogramming.net/static/images/machine-learning/linear-soft-margin-example.png\">\n",
    "We have a couple errors or violations noted by arrows, but this is likely to classify future featuresets better overall. What we have here is a **\"soft margin\"** classifier, which allows for some **\"slack\"** on the errors that we might get in the optimization process.  \n",
    "<br>\n",
    "The closer to 0 the slack is, the more \"hard-margin\" we are. The higher the slack, the more soft the margin is. If slack was 0, then we'd have a typical hard-margin classifier. As you might guess, however, we'd like to ideally minimize slack. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
