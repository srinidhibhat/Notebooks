{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Methodology for the K Means algorithm\n",
    "1. Choose value for K\n",
    "2. Randomly select K featuresets to start as your centroids\n",
    "3. Calculate distance of all other featuresets to centroids\n",
    "4. Classify other featuresets as same as closest centroid\n",
    "5. Take mean of each class (mean of all featuresets by class), making that mean the new centroid\n",
    "6. Repeat steps 3-5 until optimized (centroids no longer moving)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets start with imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import style\n",
    "style.use('ggplot')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.array([[1, 2],\n",
    "              [1.5, 1.8],\n",
    "              [5, 8 ],\n",
    "              [8, 8],\n",
    "              [1, 0.6],\n",
    "              [9,11]])\n",
    "\n",
    "plt.scatter(X[:,0], X[:,1], s=150)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets build our own K-Means algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class K_Means:\n",
    "    def __init__(self, k=2, tol=0.001, max_iter=300):\n",
    "        self.k = k\n",
    "        self.tol = tol\n",
    "        self.max_iter = max_iter\n",
    "        \n",
    "    def fit(self, data):\n",
    "        self.centroids = {}\n",
    "        \n",
    "        # selecting first two data samples from our dataset as centroids\n",
    "        for i in range(self.k):\n",
    "            self.centroids[i] = data[i]\n",
    "            \n",
    "        \n",
    "        for i in range(self.max_iter):\n",
    "            self.classifications = {}\n",
    "            \n",
    "            # creating two dict keys for two classifications (clusters)\n",
    "            for i in range(self.k):\n",
    "                self.classifications[i] = []\n",
    "                \n",
    "            # now, we need to iterate through our features, calculate distances of the \n",
    "            # features to the current centroids, and classify them as such\n",
    "            for featureset in data:\n",
    "                distances = [np.linalg.norm(featureset - self.centroids[centroid]) for centroid in self.centroids]\n",
    "                classification = distances.index(min(distances))\n",
    "                self.classifications[classification].append(featureset)\n",
    "                \n",
    "            # now, we're going to need to create the new centroids, as well as measuring \n",
    "            # the movement of the centroids. \n",
    "            # if that movement is less than our tolerance (self.tol), then we're all set. \n",
    "            \n",
    "            prev_centroids = dict(self.centroids)\n",
    "            \n",
    "            for classification in self.classifications:\n",
    "                self.centroids[classification] = np.average(self.classifications[classification], axis=0)\n",
    "                \n",
    "                \n",
    "            # now that we have new centroids, and knowledge of the previous centroids, \n",
    "            # we're curious if we're optimized yet. \n",
    "            \n",
    "            # we start off assuming we are optimized. \n",
    "            # we then take all of the centroids, and compare them to the previous centroids. \n",
    "            # If they are within our required tolerance, then we're happy. \n",
    "            # if not, set optimization = False and continue with next iteration\n",
    "            optimized = True\n",
    "            \n",
    "            for centroid in self.centroids:\n",
    "                original_centroid = prev_centroids[centroid]\n",
    "                current_centroid = self.centroids[centroid]\n",
    "                \n",
    "                if np.sum((current_centroid - original_centroid) / original_centroid * 100.0) > self.tol:\n",
    "                    optimized = False\n",
    "                    \n",
    "            if optimized:\n",
    "                break\n",
    "                \n",
    "    # now we can add some sort of prediction method\n",
    "    def predict(self, data):\n",
    "        distances = [np.linalg.norm(data - self.centroids[centroid]) for centroid in self.centroids]\n",
    "        classification = distances.index(min(distances))\n",
    "        \n",
    "        return classification         "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training\n",
    "Lets fit the data to our classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = K_Means()\n",
    "clf.fit(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lets plot the clusters along with dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "colors = [\"g\",\"r\"]\n",
    "\n",
    "for centroid in clf.centroids:\n",
    "    plt.scatter(clf.centroids[centroid][0], clf.centroids[centroid][1], \n",
    "                marker='o', color='k', s=150, linewidths=5)\n",
    "\n",
    "for classification in clf.classifications:\n",
    "    color = colors[classification]\n",
    "    \n",
    "    for featureset in clf.classifications[classification]:\n",
    "        plt.scatter(featureset[0], featureset[1], marker='x', color=color, \n",
    "                    s=150, linewidths=5)\n",
    "        \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing\n",
    "Lets test our classifier with some new data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---same as above--->{\n",
    "colors = [\"g\",\"r\"]\n",
    "\n",
    "for centroid in clf.centroids:\n",
    "    plt.scatter(clf.centroids[centroid][0], clf.centroids[centroid][1], \n",
    "                marker='o', color='k', s=150, linewidths=5)\n",
    "\n",
    "for classification in clf.classifications:\n",
    "    color = colors[classification]\n",
    "    \n",
    "    for featureset in clf.classifications[classification]:\n",
    "        plt.scatter(featureset[0], featureset[1], marker='x', color=color, \n",
    "                    s=150, linewidths=5)\n",
    "# }<---same as above---\n",
    "\n",
    "unknowns = np.array([[1,3],\n",
    "                     [8,9],\n",
    "                     [0,3],\n",
    "                     [5,4],\n",
    "                     [6,4],])\n",
    "\n",
    "for unknown in unknowns:\n",
    "    classification = clf.predict(unknown)\n",
    "    plt.scatter(unknown[0], unknown[1], marker=\"*\", color=colors[classification], s=150, linewidths=5)\n",
    "\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Now lets test our K-Means algorithm with Titanic Dataset and see the accuracy for ourselves"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn import preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reading data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_excel('data/titanic.xls')\n",
    "df.drop(['body','name'], 1, inplace=True)\n",
    "df.fillna(0,inplace=True)\n",
    "# df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cleaning Data\n",
    "(Below code is explained in `K-Means_TitanicDataset.ipynb` notebook)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def handle_non_numerical_data(df):\n",
    "    \n",
    "    # handling non-numerical data: must convert.\n",
    "    columns = df.columns.values\n",
    "\n",
    "    for column in columns:\n",
    "        text_digit_vals = {}\n",
    "        def convert_to_int(val):\n",
    "            return text_digit_vals[val]\n",
    "\n",
    "        #print(column,df[column].dtype)\n",
    "        if df[column].dtype != np.int64 and df[column].dtype != np.float64:\n",
    "            \n",
    "            column_contents = df[column].values.tolist()\n",
    "            #finding just the uniques\n",
    "            unique_elements = set(column_contents)\n",
    "            # great, found them. \n",
    "            x = 0\n",
    "            for unique in unique_elements:\n",
    "                if unique not in text_digit_vals:\n",
    "                    # creating dict that contains new\n",
    "                    # id per unique string\n",
    "                    text_digit_vals[unique] = x\n",
    "                    x+=1\n",
    "            # now we map the new \"id\" vlaue\n",
    "            # to replace the string. \n",
    "            df[column] = list(map(convert_to_int,df[column]))\n",
    "\n",
    "    return df\n",
    "\n",
    "df = handle_non_numerical_data(df)\n",
    "df.drop(['ticket','home.dest'], 1, inplace=True)\n",
    "# df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Separate data and labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.array(df.drop(['survived'], 1).astype(float))\n",
    "X = preprocessing.scale(X)\n",
    "y = np.array(df['survived'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = K_Means()\n",
    "clf.fit(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.24904507257448433\n"
     ]
    }
   ],
   "source": [
    "correct = 0\n",
    "\n",
    "for i in range(len(X)):\n",
    "    predict_me = np.array(X[i].astype(float))\n",
    "    predict_me = predict_me.reshape(-1, len(predict_me))\n",
    "    prediction = clf.predict(predict_me)\n",
    "    if prediction == y[i]:\n",
    "        correct += 1\n",
    "\n",
    "print(correct/len(X))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
